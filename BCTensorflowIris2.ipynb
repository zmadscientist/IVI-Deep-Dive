{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Citation:\n\nhttps://gist.github.com/katsugeneration/b492a548aa342ce0badfe1eed86ea5a4\n\nkatsugeneration/iris_tensorflow.py", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport math\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\nIRIS_DATA_SIZE = 4\nCLASS_SIZE = 3\n\n# load csv data for iris data format\ndef load_csv(filename):\n    file = pd.read_csv(filename, header=0)\n\n    # get sample's metadata\n    n_samples = int(file.columns[0])\n    n_features = int(file.columns[1])\n\n    # divide samples into explanation variables and target variable\n    data = np.empty((n_samples, n_features))\n    target = np.empty((n_samples,), dtype=np.int)\n    for i, row in enumerate(file.itertuples()):\n        target[i] = np.asarray(row[-1], dtype=np.int)\n        data[i] = np.asarray(row[1:n_features+1], dtype=np.float64)\n    return (data, target)\n\n# output train data \ndef get_batch_data(x_train, y_train, size=None):\n    if size is None:\n        size = len(x_train)\n    batch_xs = x_train\n    batch_ys = []\n\n    # convert to 1-of-N vector\n    for i in range(len(y_train)):\n        val = np.zeros((3), dtype=np.float64)\n        val[y_train[i]] = 1.0\n        batch_ys.append(val)\n    batch_ys = np.asarray(batch_ys)\n    return batch_xs[:size], batch_ys[:size]\n\n# output test data\ndef get_test_data(x_test, y_test):\n    batch_ys = []\n\n    # convert to 1-of-N vector\n    for i in range(len(y_test)):\n        val = np.zeros((3), dtype=np.float64)\n        val[y_test[i]] = 1.0\n        batch_ys.append(val)\n    return x_test, np.asarray(batch_ys)\n\n# for parameter initialize\ndef get_stddev(in_dim, out_dim):\n    return 1.3 / math.sqrt(float(in_dim) + float(out_dim))\n\n# DNN Model Class\nclass Classifier:\n    def __init__(self, hidden_units=[10], n_classes=0):\n        self._hidden_units = hidden_units\n        self._n_classes = n_classes\n        self._sess = tf.Session()\n\n    # build model\n    def inference(self, x):\n        hidden = []\n\n        # Input Layer\n        with tf.name_scope(\"input\"):\n            weights = tf.Variable(tf.truncated_normal([IRIS_DATA_SIZE, self._hidden_units[0]], stddev=get_stddev(IRIS_DATA_SIZE, self._hidden_units[0])), name='weights')\n            biases = tf.Variable(tf.zeros([self._hidden_units[0]]), name='biases')\n            input = tf.matmul(x, weights) + biases\n\n        # Hidden Layers\n        for index, num_hidden in enumerate(self._hidden_units):\n            if index == len(self._hidden_units) - 1: break\n            with tf.name_scope(\"hidden{}\".format(index+1)):\n                weights = tf.Variable(tf.truncated_normal([num_hidden, self._hidden_units[index+1]], stddev=get_stddev(num_hidden, self._hidden_units[index+1])), name='weights')\n                biases = tf.Variable(tf.zeros([self._hidden_units[index+1]]), name='biases')\n                inputs = input if index == 0 else hidden[index-1]\n                hidden.append(tf.nn.relu(tf.matmul(inputs, weights) + biases, name=\"hidden{}\".format(index+1)))\n        \n        # Output Layer\n        with tf.name_scope('output'):\n            weights = tf.Variable(tf.truncated_normal([self._hidden_units[-1], self._n_classes], stddev=get_stddev(self._hidden_units[-1], self._n_classes)), name='weights')\n            biases = tf.Variable(tf.zeros([self._n_classes]), name='biases')\n            logits = tf.nn.softmax(tf.matmul(hidden[-1], weights) + biases)\n\n        return logits\n\n    # loss function\n    def loss(self, logits, y):        \n        return -tf.reduce_mean(y * tf.log(logits))\n\n    # fitting function for train data\n    def fit(self, x_train=None, y_train=None, steps=200):\n        # build model\n        x = tf.placeholder(tf.float32, [None, IRIS_DATA_SIZE])\n        y = tf.placeholder(tf.float32, [None, CLASS_SIZE])\n        logits = self.inference(x)\n        loss = self.loss(logits, y)\n        train_op = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        # save variables\n        self._x = x\n        self._y = y\n        self._logits = logits\n \n        # init parameters\n        #init = tf.initialize_all_variables() \n        init = tf.global_variables_initializer()\n        self._sess.run(init)\n\n        # train\n        for i in range(steps):\n            batch_xs, batch_ys = get_batch_data(x_train, y_train)\n            self._sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})\n\n    # evaluation function for test data\n    def evaluate(self, x_test=None, y_test=None):\n        x_test, y_test = get_test_data(x_test, y_test)\n        \n        # build accuracy calculate step\n        correct_prediction = tf.equal(tf.argmax(self._logits, 1), tf.argmax(self._y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # evaluate\n        return self._sess.run([accuracy], feed_dict={self._x: x_test, self._y: y_test})\n\n    # label pridiction\n    def predict(self, samples):\n        predictions = tf.argmax(self._logits, 1)\n        return self._sess.run(predictions, {self._x: samples})\n\n\n# Load datasets.\nx_train, y_train = load_csv(filename=IRIS_TRAINING)\nx_test, y_test = load_csv(filename=IRIS_TEST)\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = Classifier(hidden_units=[10, 20, 10], n_classes=CLASS_SIZE)\n\n# Fit model.\nclassifier.fit(x_train, y_train, steps=200)\n\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(x_test, y_test)[0]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n\n# Classify two new flower samples.\nnew_samples = np.array([[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\ny = classifier.predict(new_samples)\nprint ('Predictions: {}'.format(str(y)))\n", 
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Accuracy: 0.933333\nPredictions: [1 2]\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "\n", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }
}